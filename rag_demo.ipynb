{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb712e8",
   "metadata": {},
   "source": [
    "## Step 1: Download and Clean the Text\n",
    "\n",
    "In this step, we download *The Art of War* by Sun Tzu from Project Gutenberg (ID: 132) and clean it for use in our RAG pipeline.  \n",
    "We remove the legal preamble and footer added by Project Gutenberg and isolate the core content starting from \"I. LAYING PLANS\".\n",
    "\n",
    "üìÑ Output: `art_of_war_cleaned.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download and clean 'The Art of War' text\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/132/132-0.txt\"\n",
    "response = requests.get(url)\n",
    "raw_text = response.text\n",
    "\n",
    "# Remove Gutenberg header/footer\n",
    "start = raw_text.find(\"I. LAYING PLANS\")\n",
    "end = raw_text.find(\"End of the Project Gutenberg\")\n",
    "cleaned_text = raw_text[start:end].strip()\n",
    "\n",
    "# Save to file\n",
    "with open(\"art_of_war_cleaned.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_text)\n",
    "\n",
    "print(\"Downloaded and saved cleaned text.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce06828d",
   "metadata": {},
   "source": [
    "## Step 2: Chunk the Document\n",
    "\n",
    "To prepare the document for vector storage, we split it into smaller overlapping text chunks.\n",
    "\n",
    "We use `RecursiveCharacterTextSplitter` from LangChain, which tries to split intelligently (e.g., at sentence or paragraph boundaries if possible).  \n",
    "This ensures better semantic coherence in each chunk.\n",
    "\n",
    "üîß Parameters:\n",
    "- `chunk_size = 500`: max characters per chunk\n",
    "- `chunk_overlap = 100`: overlap between chunks to preserve context\n",
    "\n",
    "üìÑ Input: `art_of_war_cleaned.txt`  \n",
    "üìÑ Output: A list of LangChain `Document` objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbd10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the document into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Load cleaned text\n",
    "with open(\"art_of_war_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Split using LangChain's text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# Wrap chunks into LangChain Document objects\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "print(f\"Total chunks created: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8b544",
   "metadata": {},
   "source": [
    "## Step 3: Inspect First Two Chunks\n",
    "\n",
    "Before embedding the text, we inspect the first two chunks to understand how overlapping works.\n",
    "\n",
    "We're using `chunk_size=500` and `chunk_overlap=100`, so each chunk should share ~100 characters with the previous one.  \n",
    "This helps preserve context across chunk boundaries during retrieval.\n",
    "\n",
    "This step is useful for debugging and verifying the chunking logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559625ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Print first two chunks and highlight overlap\n",
    "chunk1 = documents[0].page_content\n",
    "chunk2 = documents[1].page_content\n",
    "\n",
    "# Find the overlap manually (by matching last 100 chars of chunk1 with the start of chunk2)\n",
    "overlap = \"\"\n",
    "for i in range(100, 0, -1):\n",
    "    if chunk1[-i:] == chunk2[:i]:\n",
    "        overlap = chunk1[-i:]\n",
    "        break\n",
    "\n",
    "print(\"--- Chunk 1 ---\")\n",
    "print(chunk1)\n",
    "print(\"\\n--- Chunk 2 ---\")\n",
    "print(chunk2)\n",
    "\n",
    "print(\"\\n--- Overlap Detected ---\")\n",
    "print(overlap if overlap else \"(No overlap found)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2b609",
   "metadata": {},
   "source": [
    "## üîç Note: Why No Overlap Was Detected\n",
    "\n",
    "Even though we used `chunk_overlap=100`, no text overlap was detected between Chunk 1 and Chunk 2.  \n",
    "This is because `RecursiveCharacterTextSplitter` prioritizes **semantic breakpoints** like sentence or paragraph ends over strictly enforcing overlap.\n",
    "\n",
    "It tries to split at natural language boundaries first, and only falls back to hard slicing if needed.\n",
    "\n",
    "### üß† Takeaway:\n",
    "- Overlap **is a guideline**, not a hard rule.\n",
    "- In early chunks with clean sentence structure (like The Art of War), overlap may not be triggered.\n",
    "- This behavior improves the quality of text retrieval during RAG, since chunks are more coherent.\n",
    "\n",
    "If needed for debugging, you can switch to `CharacterTextSplitter` to force strict overlap logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70af896",
   "metadata": {},
   "source": [
    "## Step 4: Embed Chunks and Store in FAISS (via .env-configured Model)\n",
    "\n",
    "In this step, we embed each chunk into a vector using a local embedding model and store it in a FAISS index.\n",
    "\n",
    "We dynamically read the embedding model name from a `.env` file for flexibility.  \n",
    "This lets us switch between models (e.g., `nomic-embed-text`, `bge-base-en`) without changing code.\n",
    "\n",
    "### üîß How it Works:\n",
    "1. Load model name from `.env` (e.g., `EMBEDDING_MODEL=nomic-embed-text`)\n",
    "2. Use `OllamaEmbeddings` to embed each document chunk\n",
    "3. Store all vectors in FAISS\n",
    "4. Save the FAISS index to disk in `faiss_index/`\n",
    "\n",
    "üìÑ Input: `List[Document]`  \n",
    "üì¶ Output: `faiss_index/` folder with vector DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Embed and store in FAISS using Ollama + .env config (compatible with langchain 0.3.21)\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load embedding model name from .env\n",
    "load_dotenv()\n",
    "embedding_model_name = os.getenv(\"EMBEDDING_MODEL\", \"nomic-embed-text\")\n",
    "\n",
    "# Initialize embedding model from Ollama\n",
    "embedding_model = OllamaEmbeddings(model=embedding_model_name)\n",
    "\n",
    "# Embed and store in FAISS\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Save index to disk\n",
    "index_dir = \"faiss_index\"\n",
    "os.makedirs(index_dir, exist_ok=True)\n",
    "vectorstore.save_local(index_dir)\n",
    "\n",
    "print(f\"FAISS index saved to '{index_dir}' using embedding model '{embedding_model_name}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7139b376",
   "metadata": {},
   "source": [
    "## Step 5: Query the FAISS Index\n",
    "\n",
    "In this step, we simulate a user query and search the FAISS index to retrieve the most relevant document chunks.\n",
    "\n",
    "This is the **Retrieval (R)** part of RAG:\n",
    "- The query is embedded using the same model as the chunks\n",
    "- FAISS performs similarity search to find top-k matching chunks\n",
    "- The retrieved text will later be passed to an LLM for answering\n",
    "\n",
    "We‚Äôll just print the top 3 chunks for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa680603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Query the FAISS index and print top-k results (with deserialization fix)\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import os\n",
    "\n",
    "# Reload model and vector store\n",
    "embedding_model = OllamaEmbeddings(model=os.getenv(\"EMBEDDING_MODEL\", \"nomic-embed-text\"))\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"faiss_index\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# User query\n",
    "query = \"What does Sun Tzu say about deception?\"\n",
    "\n",
    "# Perform similarity search\n",
    "top_k = 3\n",
    "results = vectorstore.similarity_search(query, k=top_k)\n",
    "\n",
    "# Display results\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n--- Retrieved Chunk {i} ---\\n\")\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c277ad",
   "metadata": {},
   "source": [
    "## Step 6: Prompt Augmentation ‚Äì A in RAG\n",
    "\n",
    "Now that we‚Äôve retrieved relevant context, we build a prompt that can be passed to a language model.\n",
    "\n",
    "This is the **Augmentation (A)** step:\n",
    "- Combine the retrieved chunks into a single context block\n",
    "- Add the user‚Äôs question\n",
    "- Format everything into a clear prompt\n",
    "\n",
    "We will **not** generate a response yet ‚Äî just prepare the input to be fed to the LLM in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1837bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Build prompt from retrieved context and user question\n",
    "\n",
    "# Concatenate the content of all retrieved chunks\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "# Define the user query again\n",
    "query = \"What does Sun Tzu say about deception?\"\n",
    "\n",
    "# Build the full prompt (can be tuned later)\n",
    "prompt = f\"\"\"You are a helpful assistant. Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Print the final prompt (for inspection only)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef4258",
   "metadata": {},
   "source": [
    "## Step 7: Generate Answer ‚Äì G in RAG\n",
    "\n",
    "In this final step of the RAG pipeline, we use the augmented prompt from Step 6 and send it to a local language model.\n",
    "\n",
    "This is the **Generation (G)** step:\n",
    "- We use the `Ollama` class directly to interact with `orca-mini`\n",
    "- The prompt includes retrieved context and the user‚Äôs question\n",
    "- The model responds with a grounded, natural-language answer\n",
    "\n",
    "üß† Model: `orca-mini` (running locally via Ollama)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ac95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Invoke the model with the prompt\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"orca-mini\")\n",
    "\n",
    "# Invoke the model with our prepared prompt\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "# Print the output\n",
    "print(\"üß† Generated Answer:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "war-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
